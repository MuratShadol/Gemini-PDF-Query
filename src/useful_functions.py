from PyPDF2 import PdfReader
from src.gemini_client import GeminiClient
from src.prompts import get_main_prompt, get_additional_prompt
from langchain_text_splitters import CharacterTextSplitter
from loguru import logger


def get_pdf_text(pdf_reader: PdfReader) -> str:
    """
    Description:
    Extracts and concatenates text from all pages of a PDF file using a PdfReader object.

    Parameters:
    pdf_reader (PdfReader): An instance of PdfReader that represents the loaded PDF file.
    
    Returns:
    str: A string containing the extracted text from the PDF.

    """
    logger.info("Extracting text from file")
    text = ""
    for page in pdf_reader.pages:
        text += page.extract_text()
    return text

def get_text_chunks(text) -> list:
    """
    Description:
    Splits a given text into manageable chunks for processing, using a specified chunk size and overlap.

    Parameters:
    text (str): The text to be split into chunks.
    
    Returns:
    list: A list of text chunks, each of which is up to 1000 characters long, with a 200-character overlap.
"""
    text_splitter = CharacterTextSplitter(
        separator=" ",
        chunk_size=1000,
        chunk_overlap=200,
        length_function=len
    )
    chunks = text_splitter.split_text(text)
    return chunks

def call_gemini(chunks: list, user_message: str) -> str:
    """
    Description:
    Interacts with the Gemini API to generate responses based on the provided text chunks and a user message. It handles multiple chunks if available, updating the prompt with the previous response for context.

    Parameters:
    chunks (list): A list of text chunks to be processed.
    user_message (str): The user's input question or message.
    
    Returns:
    str: The response generated by the Gemini API based on the provided chunks and user message.
"""
    model = GeminiClient()
    prompt = get_main_prompt(question=user_message, chunk=chunks[0])    
    response = model.generate(prompt=prompt)

    if len(chunks) > 1:
        logger.info(f"There are {len(chunks)} chunks. Starting additional requests to LLM...")
        for i, chunk in enumerate(chunks):
            logger.debug(f"Handling {i} request")
            prompt = get_additional_prompt(question=user_message, chunk=chunk, prev_response=response)
            response = model.generate(prompt=prompt)
    return response


if __name__ == "__main__":
    pass